# Strategies for Labeling

## Objective
The main idea is to demonstrate how the calculations of some strategies for choosing labels in active learning (Shannon Entropy, KL Divergence, ...) are, at least in a field with simpler examples, not difficult to perform. 

## Issues
In this sense, in order to improve coding in python, I set out to solve some problems:
1. Perform the Shannon Entropy calculation on a selected data set (Problem1)
2. Perform the KL Divergence calculation on a selected data set (Problemm2)
3. Construct a simple graph with one of the results (Figure)

<p align="center">
  <img src="https://github.com/victormlb06/Strategies-for-Labeling/blob/main/Figura%20KLDivergence.png" alt="Logo moveit"/>
</p>


## Dataset
The dataset used is available on github, but was taken from https://www.kaggle.com/spscientist/students-performance-in-exams


